{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d346ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Carga y exploración inicial de datos\n",
    "\n",
    "print(\"Cargando datos eléctricos...\")\n",
    "electrical_data = pd.read_csv('data/electrical_data.csv', parse_dates=['measured_on'])\n",
    "\n",
    "print(\"Cargando datos ambientales...\")\n",
    "environment_data = pd.read_csv('data/environment_data.csv', parse_dates=['measured_on'])\n",
    "\n",
    "print(\"Cargando datos de irradiancia...\")\n",
    "irradiance_data = pd.read_csv('data/irradiance_data.csv', parse_dates=['measured_on'])\n",
    "\n",
    "# **AQUÍ VA LA SOLUCIÓN 2 - LIMPIEZA DE DATOS**\n",
    "# Limpiar y convertir fechas\n",
    "def clean_datetime_column(df, col_name):\n",
    "    # Convertir a datetime, colocando NaT para valores inválidos\n",
    "    df[col_name] = pd.to_datetime(df[col_name], errors='coerce')\n",
    "    # Eliminar filas con fechas inválidas\n",
    "    df = df.dropna(subset=[col_name])\n",
    "    return df\n",
    "\n",
    "print(\"Limpiando datos de fechas...\")\n",
    "electrical_data = clean_datetime_column(electrical_data, 'measured_on')\n",
    "environment_data = clean_datetime_column(environment_data, 'measured_on')\n",
    "irradiance_data = clean_datetime_column(irradiance_data, 'measured_on')\n",
    "\n",
    "# Verificar el contenido de la columna measured_on (OPCIONAL - puedes mantenerlo para diagnóstico)\n",
    "print(\"Tipos de datos en measured_on:\")\n",
    "print(electrical_data['measured_on'].dtype)\n",
    "print(f\"Valores únicos de tipo: {electrical_data['measured_on'].apply(type).value_counts()}\")\n",
    "\n",
    "# Ver algunos valores para identificar el problema\n",
    "print(\"\\nPrimeros valores de measured_on:\")\n",
    "print(electrical_data['measured_on'].head(10))\n",
    "\n",
    "# Mostrar información básica de los datasets\n",
    "print(\"\\nInformación del dataset de datos eléctricos:\")\n",
    "print(f\"- Dimensiones: {electrical_data.shape}\")\n",
    "print(f\"- Columnas: {len(electrical_data.columns)}\")\n",
    "if len(electrical_data) > 0:\n",
    "    print(f\"- Rango de fechas: {electrical_data['measured_on'].min().date()} a {electrical_data['measured_on'].max().date()}\")\n",
    "else:\n",
    "    print(\"- No hay datos válidos\")\n",
    "\n",
    "print(\"\\nInformación del dataset de datos ambientales:\")\n",
    "print(f\"- Dimensiones: {environment_data.shape}\")\n",
    "print(f\"- Columnas: {environment_data.columns.tolist()}\")\n",
    "if len(environment_data) > 0:\n",
    "    print(f\"- Rango de fechas: {environment_data['measured_on'].min().date()} a {environment_data['measured_on'].max().date()}\")\n",
    "else:\n",
    "    print(\"- No hay datos válidos\")\n",
    "\n",
    "print(\"\\nInformación del dataset de datos de irradiancia:\")\n",
    "print(f\"- Dimensiones: {irradiance_data.shape}\")\n",
    "print(f\"- Columnas: {irradiance_data.columns.tolist()}\")\n",
    "if len(irradiance_data) > 0:\n",
    "    print(f\"- Rango de fechas: {irradiance_data['measured_on'].min().date()} a {irradiance_data['measured_on'].max().date()}\")\n",
    "else:\n",
    "    print(\"- No hay datos válidos\")\n",
    "\n",
    "# Mostrar las primeras filas de cada dataset\n",
    "print(\"\\nPrimeras filas del dataset de datos eléctricos:\")\n",
    "display(electrical_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPrimeras filas del dataset de datos ambientales:\")\n",
    "display(environment_data.head())\n",
    "\n",
    "print(\"\\nPrimeras filas del dataset de datos de irradiancia:\")\n",
    "display(irradiance_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13573b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Procesamiento del archivo electrical_data.csv\n",
    "\n",
    "# Convertir la columna measured_on a tipo datetime\n",
    "electrical_data['measured_on'] = pd.to_datetime(electrical_data['measured_on'])\n",
    "environment_data['measured_on'] = pd.to_datetime(environment_data['measured_on'])\n",
    "irradiance_data['measured_on'] = pd.to_datetime(irradiance_data['measured_on'])\n",
    "\n",
    "# Función para reestructurar el dataset de formato ancho a formato largo\n",
    "def wide_to_long_format(df):\n",
    "    # Lista para almacenar los dataframes de cada inversor\n",
    "    inverter_dfs = []\n",
    "    \n",
    "    # Iterar sobre los 24 inversores\n",
    "    for inv_num in range(1, 25):\n",
    "        # Formatear el número del inversor con ceros a la izquierda (inv_01, inv_02, etc.)\n",
    "        inv_prefix = f\"inv_{inv_num:02d}\"\n",
    "        \n",
    "        # Filtrar columnas correspondientes a este inversor\n",
    "        inv_cols = [col for col in df.columns if inv_prefix in col]\n",
    "        \n",
    "        if not inv_cols:\n",
    "            continue\n",
    "            \n",
    "        # Seleccionar solo las columnas de este inversor y measured_on\n",
    "        inv_df = df[['measured_on'] + inv_cols].copy()\n",
    "        \n",
    "        # Crear columna device con el número de inversor\n",
    "        inv_df['device'] = inv_num\n",
    "        \n",
    "        # Renombrar columnas para eliminar el prefijo del inversor\n",
    "        rename_dict = {}\n",
    "        for col in inv_cols:\n",
    "            # Intentar extraer el tipo de medición usando el patrón esperado\n",
    "            try:\n",
    "                if f\"{inv_prefix}_\" in col:\n",
    "                    parts = col.split(f\"{inv_prefix}_\")\n",
    "                    if len(parts) > 1 and '_inv_' in parts[1]:\n",
    "                        measurement_type = parts[1].split('_inv_')[0]\n",
    "                        rename_dict[col] = measurement_type\n",
    "                    else:\n",
    "                        # Si no sigue el patrón exacto, intentar inferir el tipo\n",
    "                        if 'dc_current' in col:\n",
    "                            rename_dict[col] = 'dc_current'\n",
    "                        elif 'dc_voltage' in col:\n",
    "                            rename_dict[col] = 'dc_voltage'\n",
    "                        elif 'ac_current' in col:\n",
    "                            rename_dict[col] = 'ac_current'\n",
    "                        elif 'ac_voltage' in col:\n",
    "                            rename_dict[col] = 'ac_voltage'\n",
    "                        elif 'ac_power' in col:\n",
    "                            rename_dict[col] = 'ac_power'\n",
    "                        else:\n",
    "                            # Si no se puede inferir, usar el nombre completo\n",
    "                            rename_dict[col] = col\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando columna {col}: {e}\")\n",
    "                # En caso de error, mantener el nombre original\n",
    "                rename_dict[col] = col\n",
    "            \n",
    "        inv_df = inv_df.rename(columns=rename_dict)\n",
    "        \n",
    "        # Asegurar que las columnas numéricas sean de tipo numérico, forzando a NaN los errores\n",
    "        numeric_cols = ['dc_current', 'dc_voltage', 'ac_current', 'ac_voltage', 'ac_power']\n",
    "        for num_col in numeric_cols:\n",
    "            if num_col in inv_df.columns:\n",
    "                inv_df[num_col] = pd.to_numeric(inv_df[num_col], errors='coerce')\n",
    "\n",
    "        # Seleccionar solo las columnas necesarias\n",
    "        cols_to_keep = ['measured_on', 'device', 'dc_current', 'dc_voltage', \n",
    "                        'ac_current', 'ac_voltage', 'ac_power']\n",
    "        \n",
    "        # Filtrar solo las columnas que existen en el DataFrame\n",
    "        existing_cols = [col for col in cols_to_keep if col in inv_df.columns]\n",
    "        inv_df = inv_df[existing_cols]\n",
    "        \n",
    "        # Agregar a la lista\n",
    "        inverter_dfs.append(inv_df)\n",
    "    \n",
    "    # Concatenar todos los dataframes de inversores\n",
    "    long_format_df = pd.concat(inverter_dfs, ignore_index=True)\n",
    "    \n",
    "    return long_format_df\n",
    "\n",
    "# Aplicar la transformación\n",
    "print(\"Transformando datos eléctricos de formato ancho a largo...\")\n",
    "electrical_long = wide_to_long_format(electrical_data)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(\"\\nPrimeras filas del dataset transformado:\")\n",
    "display(electrical_long.head(10))\n",
    "print(electrical_long.tail())\n",
    "\n",
    "# Verificar la estructura del dataset resultante\n",
    "print(f\"\\nDimensiones del dataset transformado: {electrical_long.shape}\")\n",
    "print(f\"Número de inversores únicos: {electrical_long['device'].nunique()}\")\n",
    "print(f\"Columnas del dataset transformado: {electrical_long.columns.tolist()}\")\n",
    "\n",
    "# Verificar valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(electrical_long.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Unificación de datasets\n",
    "\n",
    "# Fusionar los tres datasets utilizando la columna measured_on\n",
    "print(\"Fusionando datasets...\")\n",
    "\n",
    "# Primero fusionamos electrical_long con environment_data\n",
    "merged_df = pd.merge(\n",
    "    electrical_long,\n",
    "    environment_data,\n",
    "    on='measured_on',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Luego fusionamos con irradiance_data\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    irradiance_data,\n",
    "    on='measured_on',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificar el resultado de la fusión\n",
    "print(\"\\nPrimeras filas del dataset unificado:\")\n",
    "display(merged_df.head())\n",
    "\n",
    "# Información sobre el dataset unificado\n",
    "print(f\"\\nDimensiones del dataset unificado: {merged_df.shape}\")\n",
    "print(f\"Columnas del dataset unificado: {merged_df.columns.tolist()}\")\n",
    "\n",
    "# Verificar valores nulos después de la fusión\n",
    "print(\"\\nValores nulos por columna después de la fusión:\")\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "# Eliminar filas con valores nulos si es necesario\n",
    "if merged_df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nEliminando filas con valores nulos...\")\n",
    "    merged_df = merged_df.dropna()\n",
    "    print(f\"Dimensiones después de eliminar valores nulos: {merged_df.shape}\")\n",
    "\n",
    "# Resumen estadístico del dataset unificado\n",
    "print(\"\\nResumen estadístico del dataset unificado:\")\n",
    "display(merged_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Ingeniería de características\n",
    "\n",
    "# Extraer características temporales\n",
    "print(\"Extrayendo características temporales...\")\n",
    "merged_df['hour'] = merged_df['measured_on'].dt.hour\n",
    "merged_df['day'] = merged_df['measured_on'].dt.day\n",
    "merged_df['month'] = merged_df['measured_on'].dt.month\n",
    "merged_df['year'] = merged_df['measured_on'].dt.year\n",
    "merged_df['dayofweek'] = merged_df['measured_on'].dt.dayofweek\n",
    "\n",
    "# 1. Calcular la eficiencia de conversión\n",
    "print(\"\\nCalculando eficiencia de conversión...\")\n",
    "# Evitar divisiones por cero\n",
    "mask = (merged_df['dc_current'] > 0) & (merged_df['dc_voltage'] > 0)\n",
    "merged_df['efficiency'] = 0.0  # Valor predeterminado\n",
    "\n",
    "# Calcular solo donde dc_current y dc_voltage son mayores que cero\n",
    "merged_df.loc[mask, 'efficiency'] = (\n",
    "    merged_df.loc[mask, 'ac_power'] / \n",
    "    (merged_df.loc[mask, 'dc_current'] * merged_df.loc[mask, 'dc_voltage'])\n",
    ")\n",
    "\n",
    "# Limitar la eficiencia a un máximo de 1.0 (100%)\n",
    "merged_df['efficiency'] = merged_df['efficiency'].clip(upper=1.0)\n",
    "\n",
    "# 2. Calcular la estabilidad DC\n",
    "print(\"Calculando estabilidad DC...\")\n",
    "\n",
    "# Agrupar por device para calcular la estabilidad\n",
    "stability_df = merged_df.groupby('device').agg({\n",
    "    'dc_voltage': ['mean', 'std'],\n",
    "    'dc_current': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Aplanar la estructura de columnas\n",
    "stability_df.columns = ['device', 'dc_voltage_mean', 'dc_voltage_std', 'dc_current_mean', 'dc_current_std']\n",
    "\n",
    "# Calcular la estabilidad como 1 - (std/mean)\n",
    "stability_df['stability_dc_voltage'] = 1 - (stability_df['dc_voltage_std'] / stability_df['dc_voltage_mean']).fillna(0)\n",
    "stability_df['stability_dc_current'] = 1 - (stability_df['dc_current_std'] / stability_df['dc_current_mean']).fillna(0)\n",
    "\n",
    "# Fusionar la estabilidad con el dataset principal\n",
    "merged_df = pd.merge(merged_df, stability_df[['device', 'stability_dc_voltage', 'stability_dc_current']], on='device', how='left')\n",
    "\n",
    "# 3. Calcular el impacto ambiental sobre la corriente\n",
    "print(\"Analizando impacto ambiental sobre la corriente DC...\")\n",
    "\n",
    "# Crear una copia del dataframe para análisis\n",
    "env_impact_df = merged_df.copy()\n",
    "\n",
    "# Mostrar correlaciones entre variables ambientales y dc_current\n",
    "corr_matrix = env_impact_df[['dc_current', 'ambient_temperature_o_149575', \n",
    "                            'poa_irradiance_o_149574', 'wind_speed_o_149576']].corr()\n",
    "\n",
    "print(\"\\nMatriz de correlación entre variables ambientales y corriente DC:\")\n",
    "display(corr_matrix)\n",
    "\n",
    "# Visualizar la correlación\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlación entre variables ambientales y corriente DC')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crear una métrica de impacto ambiental basada en las correlaciones\n",
    "# Normalizar las variables ambientales\n",
    "scaler = StandardScaler()\n",
    "env_vars = ['ambient_temperature_o_149575', 'poa_irradiance_o_149574', 'wind_speed_o_149576']\n",
    "env_impact_df[env_vars] = scaler.fit_transform(env_impact_df[env_vars])\n",
    "\n",
    "# Crear la métrica de impacto ambiental como una combinación ponderada\n",
    "# basada en las correlaciones observadas\n",
    "env_impact_df['env_impact'] = (\n",
    "    env_impact_df['poa_irradiance_o_149574'] * 0.7 +  # Mayor peso a la irradiancia\n",
    "    env_impact_df['ambient_temperature_o_149575'] * 0.2 +  # Peso medio a la temperatura\n",
    "    env_impact_df['wind_speed_o_149576'] * 0.1  # Menor peso a la velocidad del viento\n",
    ")\n",
    "\n",
    "# Fusionar esta métrica con el dataset principal\n",
    "merged_df['env_impact'] = env_impact_df['env_impact']\n",
    "\n",
    "# Mostrar las primeras filas del dataset con las nuevas características\n",
    "print(\"\\nPrimeras filas del dataset con características derivadas:\")\n",
    "display(merged_df.head())\n",
    "\n",
    "# Resumen estadístico de las nuevas características\n",
    "print(\"\\nResumen estadístico de las características derivadas:\")\n",
    "display(merged_df[['efficiency', 'stability_dc_voltage', 'stability_dc_current', 'env_impact']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd250f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Generación de cubos OLAP\n",
    "\n",
    "print(\"Generando cubos OLAP...\")\n",
    "\n",
    "# Preparar los datos para los cubos OLAP\n",
    "olap_data = merged_df.copy()\n",
    "\n",
    "# 1. Cubo OLAP para eficiencia y estabilidad DC por device, hora y fecha\n",
    "print(\"\\n1. Cubo OLAP: Eficiencia y estabilidad DC por device, hora y fecha\")\n",
    "\n",
    "# Crear el cubo OLAP utilizando pivot_table\n",
    "efficiency_stability_cube = pd.pivot_table(\n",
    "    olap_data,\n",
    "    values=['efficiency', 'stability_dc_voltage', 'stability_dc_current'],\n",
    "    index=['device'],\n",
    "    columns=['hour'],\n",
    "    aggfunc={\n",
    "        'efficiency': ['mean', 'std', 'min', 'max'],\n",
    "        'stability_dc_voltage': ['mean'],\n",
    "        'stability_dc_current': ['mean']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Mostrar el cubo\n",
    "print(\"\\nCubo OLAP de eficiencia y estabilidad DC:\")\n",
    "display(efficiency_stability_cube.head())\n",
    "\n",
    "# Visualización del cubo: Eficiencia media por dispositivo y hora\n",
    "plt.figure(figsize=(14, 8))\n",
    "efficiency_by_hour_device = pd.pivot_table(\n",
    "    olap_data,\n",
    "    values='efficiency',\n",
    "    index='hour',\n",
    "    columns='device',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(efficiency_by_hour_device, annot=False, cmap='viridis')\n",
    "plt.title('Eficiencia media por dispositivo y hora del día')\n",
    "plt.xlabel('Dispositivo (ID)')\n",
    "plt.ylabel('Hora del día')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Cubo OLAP para impacto ambiental sobre el rendimiento de cada inversor\n",
    "print(\"\\n2. Cubo OLAP: Impacto ambiental sobre el rendimiento de inversores\")\n",
    "\n",
    "# Discretizar variables ambientales para facilitar el análisis\n",
    "# Para temperatura ambiente\n",
    "try:\n",
    "    olap_data['temp_bin'] = pd.qcut(olap_data['ambient_temperature_o_149575'], 4, \n",
    "                                labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'],\n",
    "                                duplicates='drop')\n",
    "except ValueError:\n",
    "    # Si hay valores duplicados, usar cut con bins personalizados\n",
    "    temp_min = olap_data['ambient_temperature_o_149575'].min()\n",
    "    temp_max = olap_data['ambient_temperature_o_149575'].max()\n",
    "    temp_bins = np.linspace(temp_min, temp_max, 5)  # 5 bordes para 4 intervalos\n",
    "    olap_data['temp_bin'] = pd.cut(olap_data['ambient_temperature_o_149575'], \n",
    "                                bins=temp_bins,\n",
    "                                labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'],\n",
    "                                include_lowest=True)\n",
    "\n",
    "# Para irradiancia solar (que probablemente tiene muchos valores cero)\n",
    "# Usar cut en lugar de qcut para evitar problemas con valores duplicados\n",
    "irr_min = olap_data['poa_irradiance_o_149574'].min()\n",
    "irr_max = olap_data['poa_irradiance_o_149574'].max()\n",
    "\n",
    "# Crear bins personalizados para manejar mejor la distribución de irradiancia\n",
    "# (que suele tener muchos ceros durante la noche)\n",
    "irr_bins = [irr_min, 1, irr_max*0.33, irr_max*0.66, irr_max] \n",
    "olap_data['irradiance_bin'] = pd.cut(olap_data['poa_irradiance_o_149574'], \n",
    "                                bins=irr_bins,\n",
    "                                labels=['Nula/Muy Baja', 'Baja', 'Media', 'Alta'],\n",
    "                                include_lowest=True)\n",
    "\n",
    "# Para velocidad del viento\n",
    "try:\n",
    "    olap_data['wind_bin'] = pd.qcut(olap_data['wind_speed_o_149576'], 4, \n",
    "                                labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'],\n",
    "                                duplicates='drop')\n",
    "except ValueError:\n",
    "    # Si hay valores duplicados, usar cut con bins personalizados\n",
    "    wind_min = olap_data['wind_speed_o_149576'].min()\n",
    "    wind_max = olap_data['wind_speed_o_149576'].max()\n",
    "    wind_bins = np.linspace(wind_min, wind_max, 5)  # 5 bordes para 4 intervalos\n",
    "    olap_data['wind_bin'] = pd.cut(olap_data['wind_speed_o_149576'], \n",
    "                                bins=wind_bins,\n",
    "                                labels=['Baja', 'Media-Baja', 'Media-Alta', 'Alta'],\n",
    "                                include_lowest=True)\n",
    "\n",
    "# Mostrar la distribución de las variables discretizadas\n",
    "print(\"\\nDistribución de las variables discretizadas:\")\n",
    "print(\"Temperatura:\")\n",
    "print(olap_data['temp_bin'].value_counts())\n",
    "print(\"\\nIrradiancia:\")\n",
    "print(olap_data['irradiance_bin'].value_counts())\n",
    "print(\"\\nVelocidad del viento:\")\n",
    "print(olap_data['wind_bin'].value_counts())\n",
    "\n",
    "# Crear el cubo OLAP para impacto ambiental\n",
    "env_impact_cube = pd.pivot_table(\n",
    "    olap_data,\n",
    "    values=['dc_current', 'efficiency', 'env_impact'],\n",
    "    index=['device'],\n",
    "    columns=['irradiance_bin', 'temp_bin'],\n",
    "    aggfunc={\n",
    "        'dc_current': ['mean', 'std'],\n",
    "        'efficiency': ['mean'],\n",
    "        'env_impact': ['mean']\n",
    "    }\n",
    ")\n",
    "\n",
    "# Mostrar el cubo\n",
    "print(\"\\nCubo OLAP de impacto ambiental:\")\n",
    "display(env_impact_cube.head())\n",
    "\n",
    "# Visualización del impacto de la irradiancia en la corriente DC por dispositivo\n",
    "plt.figure(figsize=(14, 8))\n",
    "dc_current_by_irradiance = pd.pivot_table(\n",
    "    olap_data,\n",
    "    values='dc_current',\n",
    "    index='device',\n",
    "    columns='irradiance_bin',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(dc_current_by_irradiance, annot=True, cmap='YlOrRd', fmt='.2f')\n",
    "plt.title('Impacto de la irradiancia en la corriente DC por dispositivo')\n",
    "plt.xlabel('Nivel de irradiancia')\n",
    "plt.ylabel('Dispositivo (ID)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar los cubos OLAP para uso posterior si es necesario\n",
    "efficiency_stability_cube.to_csv('efficiency_stability_cube.csv')\n",
    "env_impact_cube.to_csv('env_impact_cube.csv')\n",
    "\n",
    "print(\"\\nCubos OLAP generados y guardados correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar características relevantes para la detección de anomalías\n",
    "features = [\n",
    "    'dc_current', 'dc_voltage', 'ac_current', 'ac_voltage', 'ac_power',\n",
    "    'ambient_temperature_o_149575', 'wind_speed_o_149576', 'poa_irradiance_o_149574',\n",
    "    'efficiency', 'stability_dc_voltage', 'stability_dc_current', 'env_impact'\n",
    "]\n",
    "\n",
    "# Filtrar datos para tener solo filas con valores válidos\n",
    "anomaly_data_full = merged_df.dropna(subset=features).copy()\n",
    "\n",
    "# Tomar una muestra pequeña pero representativa (5000 filas)\n",
    "print(f\"Tomando una muestra de 5000 filas de {len(anomaly_data_full)} para acelerar el análisis...\")\n",
    "anomaly_data = anomaly_data_full.sample(n=5000, random_state=42)\n",
    "\n",
    "# Normalizar los datos\n",
    "print(\"Normalizando datos...\")\n",
    "scaler = StandardScaler()\n",
    "anomaly_data_scaled = scaler.fit_transform(anomaly_data[features])\n",
    "\n",
    "# Crear un DataFrame con los datos escalados y mantener el mismo índice que anomaly_data\n",
    "anomaly_df = pd.DataFrame(anomaly_data_scaled, columns=features, index=anomaly_data.index)\n",
    "\n",
    "# Añadir la columna device para análisis posterior\n",
    "anomaly_df['device'] = anomaly_data['device'].values\n",
    "\n",
    "print(f\"Datos preparados: {anomaly_df.shape[0]} filas, {anomaly_df.shape[1]} columnas\")\n",
    "\n",
    "# 1. Modelo Isolation Forest (más rápido y eficiente para grandes conjuntos de datos)\n",
    "print(\"\\n1. Entrenando modelo Isolation Forest...\")\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=0.05,  # Asumimos que aproximadamente el 5% de los datos son anomalías\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles para acelerar\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "iso_forest.fit(anomaly_data_scaled)\n",
    "\n",
    "# Predecir anomalías\n",
    "anomaly_df['anomaly_iforest'] = iso_forest.predict(anomaly_data_scaled)\n",
    "# Convertir a formato binario (1: normal, 0: anomalía)\n",
    "anomaly_df['anomaly_iforest'] = anomaly_df['anomaly_iforest'].map({1: 0, -1: 1})\n",
    "\n",
    "# 2. Usamos DBSCAN en lugar de One-Class SVM (mucho más rápido)\n",
    "print(\"2. Aplicando DBSCAN (más rápido que One-Class SVM)...\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "dbscan_labels = dbscan.fit_predict(anomaly_data_scaled)\n",
    "\n",
    "# En DBSCAN, -1 son outliers (anomalías)\n",
    "anomaly_df['anomaly_dbscan'] = (dbscan_labels == -1).astype(int)\n",
    "\n",
    "# 3. Modelo Local Outlier Factor (LOF) con menos vecinos para acelerar\n",
    "print(\"3. Aplicando Local Outlier Factor...\")\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=10,  # Menos vecinos = más rápido\n",
    "    contamination=0.05,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Predecir anomalías (LOF es un detector de anomalías no supervisado)\n",
    "anomaly_df['anomaly_lof'] = lof.fit_predict(anomaly_data_scaled)\n",
    "# Convertir a formato binario (1: normal, 0: anomalía)\n",
    "anomaly_df['anomaly_lof'] = anomaly_df['anomaly_lof'].map({1: 0, -1: 1})\n",
    "\n",
    "# Combinar resultados de los tres modelos\n",
    "anomaly_df['anomaly_score'] = anomaly_df['anomaly_iforest'] + anomaly_df['anomaly_dbscan'] + anomaly_df['anomaly_lof']\n",
    "# Una puntuación de 2 o más significa que al menos 2 de los 3 modelos detectaron una anomalía\n",
    "anomaly_df['is_anomaly'] = (anomaly_df['anomaly_score'] >= 2).astype(int)\n",
    "\n",
    "# Añadir la columna is_anomaly al dataset original\n",
    "anomaly_data['is_anomaly'] = anomaly_df['is_anomaly']\n",
    "\n",
    "# Mostrar el número de anomalías detectadas\n",
    "total_anomalies = anomaly_df['is_anomaly'].sum()\n",
    "print(f\"\\nTotal de anomalías detectadas: {total_anomalies} ({total_anomalies / len(anomaly_df) * 100:.2f}%)\")\n",
    "\n",
    "# Mostrar la distribución de anomalías por dispositivo\n",
    "anomalies_by_device = anomaly_data.groupby('device')['is_anomaly'].sum().sort_values(ascending=False)\n",
    "print(\"\\nDistribución de anomalías por dispositivo:\")\n",
    "display(anomalies_by_device)\n",
    "\n",
    "# Visualización de anomalías\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(anomalies_by_device.index, anomalies_by_device.values)\n",
    "plt.title('Número de anomalías detectadas por dispositivo')\n",
    "plt.xlabel('ID del dispositivo')\n",
    "plt.ylabel('Número de anomalías')\n",
    "plt.xticks(anomalies_by_device.index)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1427e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Estrategia de validación\n",
    "\n",
    "print(\"Aplicando estrategias de validación para el modelo no supervisado...\")\n",
    "\n",
    "# 1. Visualización mediante reducción de dimensionalidad\n",
    "print(\"\\n1. Visualización mediante reducción de dimensionalidad\")\n",
    "\n",
    "# PCA para reducir a 2 dimensiones\n",
    "print(\"Aplicando PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(anomaly_data_scaled)\n",
    "\n",
    "# Crear un DataFrame con los resultados de PCA\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['is_anomaly'] = anomaly_df['is_anomaly'].values\n",
    "pca_df['device'] = anomaly_data['device'].values\n",
    "\n",
    "# Visualizar PCA\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(\n",
    "    x='PC1', y='PC2',\n",
    "    hue='is_anomaly',\n",
    "    palette={0: 'blue', 1: 'red'},\n",
    "    data=pca_df,\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.title('Visualización de anomalías con PCA')\n",
    "plt.legend(title='Anomalía', labels=['Normal', 'Anomalía'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# t-SNE para visualización no lineal\n",
    "print(\"Aplicando t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "tsne_result = tsne.fit_transform(anomaly_data_scaled)\n",
    "\n",
    "# Crear un DataFrame con los resultados de t-SNE\n",
    "tsne_df = pd.DataFrame(data=tsne_result, columns=['t-SNE1', 't-SNE2'])\n",
    "tsne_df['is_anomaly'] = anomaly_df['is_anomaly'].values\n",
    "tsne_df['device'] = anomaly_data['device'].values\n",
    "\n",
    "# Visualizar t-SNE\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(\n",
    "    x='t-SNE1', y='t-SNE2',\n",
    "    hue='is_anomaly',\n",
    "    palette={0: 'blue', 1: 'red'},\n",
    "    data=tsne_df,\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.title('Visualización de anomalías con t-SNE')\n",
    "plt.legend(title='Anomalía', labels=['Normal', 'Anomalía'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Validación con reglas físicas\n",
    "print(\"\\n2. Validación con reglas físicas\")\n",
    "\n",
    "# Añadir las columnas originales al dataframe de anomalías para validación\n",
    "validation_df = anomaly_data.copy()\n",
    "\n",
    "# Regla 1: Eficiencia imposible (> 1.0)\n",
    "validation_df['rule_impossible_efficiency'] = (validation_df['efficiency'] > 1.0).astype(int)\n",
    "\n",
    "# Regla 2: Potencia AC sin corriente DC\n",
    "validation_df['rule_ac_without_dc'] = ((validation_df['ac_power'] > 0) & \n",
    "                                    (validation_df['dc_current'] <= 0)).astype(int)\n",
    "\n",
    "# Regla 3: Alta irradiancia pero baja corriente DC\n",
    "validation_df['rule_high_irr_low_current'] = ((validation_df['poa_irradiance_o_149574'] > validation_df['poa_irradiance_o_149574'].quantile(0.75)) & \n",
    "                                    (validation_df['dc_current'] < validation_df['dc_current'].quantile(0.25))).astype(int)\n",
    "\n",
    "# Combinar las reglas\n",
    "validation_df['rules_anomaly'] = ((validation_df['rule_impossible_efficiency'] | \n",
    "                                validation_df['rule_ac_without_dc'] | \n",
    "                                validation_df['rule_high_irr_low_current']) & \n",
    "                            (validation_df['is_anomaly'] == 1)).astype(int)\n",
    "\n",
    "# Calcular la coincidencia entre las reglas físicas y las anomalías detectadas\n",
    "rules_match = (validation_df['rules_anomaly'] == 1).sum()\n",
    "total_anomalies = (validation_df['is_anomaly'] == 1).sum()\n",
    "\n",
    "if total_anomalies > 0:\n",
    "    print(f\"Anomalías validadas por reglas físicas: {rules_match} de {total_anomalies} ({rules_match/total_anomalies*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Anomalías validadas por reglas físicas: {rules_match} de {total_anomalies} (N/A - No se detectaron anomalías)\")\n",
    "\n",
    "# Mostrar ejemplos de anomalías validadas por reglas físicas\n",
    "print(\"\\nEjemplos de anomalías validadas por reglas físicas:\")\n",
    "display(validation_df[validation_df['rules_anomaly'] == 1].head())\n",
    "\n",
    "# 3. Análisis de anomalías en condiciones ambientales normales\n",
    "print(\"\\n3. Análisis de anomalías en condiciones ambientales normales\")\n",
    "\n",
    "# Definir condiciones ambientales normales (entre percentiles 25 y 75)\n",
    "normal_conditions = (\n",
    "    (validation_df['ambient_temperature_o_149575'] >= validation_df['ambient_temperature_o_149575'].quantile(0.25)) &\n",
    "    (validation_df['ambient_temperature_o_149575'] <= validation_df['ambient_temperature_o_149575'].quantile(0.75)) &\n",
    "    (validation_df['poa_irradiance_o_149574'] >= validation_df['poa_irradiance_o_149574'].quantile(0.25)) &\n",
    "    (validation_df['poa_irradiance_o_149574'] <= validation_df['poa_irradiance_o_149574'].quantile(0.75)) &\n",
    "    (validation_df['wind_speed_o_149576'] >= validation_df['wind_speed_o_149576'].quantile(0.25)) &\n",
    "    (validation_df['wind_speed_o_149576'] <= validation_df['wind_speed_o_149576'].quantile(0.75))\n",
    ")\n",
    "\n",
    "# Contar anomalías en condiciones normales\n",
    "normal_anomalies = (validation_df['is_anomaly'] & normal_conditions).sum()\n",
    "normal_total = normal_conditions.sum()\n",
    "\n",
    "if normal_total > 0:\n",
    "    print(f\"Anomalías en condiciones ambientales normales: {normal_anomalies} de {normal_total} ({normal_anomalies/normal_total*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Anomalías en condiciones ambientales normales: {normal_anomalies} de {normal_total} (N/A - No hay datos en condiciones normales)\")\n",
    "\n",
    "# Mostrar ejemplos de anomalías en condiciones normales\n",
    "print(\"\\nEjemplos de anomalías en condiciones ambientales normales:\")\n",
    "display(validation_df[(validation_df['is_anomaly'] == 1) & normal_conditions].head())\n",
    "\n",
    "# Guardar resultados de anomalías para análisis posterior\n",
    "validation_df.to_csv('anomalies_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19edd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('dataset_final.csv', index=False)\n",
    "print(\"\\nDataset final guardado como 'dataset_final.csv'\")\n",
    "print(\"Análisis completado con éxito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
